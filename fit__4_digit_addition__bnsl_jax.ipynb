{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Install kfac_jax\n",
        "\n",
        "# need version of kfac_jax from after September 13 due to:\n",
        "# https://github.com/google-deepmind/kfac-jax/issues/142\n",
        "\n",
        "!pip uninstall kfac-jax -y\n",
        "!pip install git+https://github.com/google-deepmind/kfac-jax@d3643a1ad85cd34ce9ec096a64c5a44708743217"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37OI10kAEQln",
        "outputId": "6460ba8e-89cb-4c9d-d48f-bb35ef8376ea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping kfac-jax as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/google-deepmind/kfac-jax@d3643a1ad85cd34ce9ec096a64c5a44708743217\n",
            "  Cloning https://github.com/google-deepmind/kfac-jax (to revision d3643a1ad85cd34ce9ec096a64c5a44708743217) to /tmp/pip-req-build-2cbwtl7b\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/google-deepmind/kfac-jax /tmp/pip-req-build-2cbwtl7b\n",
            "  Running command git rev-parse -q --verify 'sha^d3643a1ad85cd34ce9ec096a64c5a44708743217'\n",
            "  Running command git fetch -q https://github.com/google-deepmind/kfac-jax d3643a1ad85cd34ce9ec096a64c5a44708743217\n",
            "  Running command git checkout -q d3643a1ad85cd34ce9ec096a64c5a44708743217\n",
            "  Resolved https://github.com/google-deepmind/kfac-jax to commit d3643a1ad85cd34ce9ec096a64c5a44708743217\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from kfac-jax==0.0.5) (1.4.0)\n",
            "Collecting immutabledict>=2.2.1 (from kfac-jax==0.0.5)\n",
            "  Downloading immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from kfac-jax==0.0.5) (1.25.2)\n",
            "Collecting distrax>=0.1.3 (from kfac-jax==0.0.5)\n",
            "  Downloading distrax-0.1.5-py3-none-any.whl (319 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jax>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from kfac-jax==0.0.5) (0.4.23)\n",
            "Requirement already satisfied: jaxlib>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from kfac-jax==0.0.5) (0.4.23+cuda12.cudnn89)\n",
            "Requirement already satisfied: dm-tree>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from kfac-jax==0.0.5) (0.1.8)\n",
            "Requirement already satisfied: optax>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from kfac-jax==0.0.5) (0.2.1)\n",
            "Requirement already satisfied: chex>=0.1.8 in /usr/local/lib/python3.10/dist-packages (from distrax>=0.1.3->kfac-jax==0.0.5) (0.1.86)\n",
            "Requirement already satisfied: tensorflow-probability>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from distrax>=0.1.3->kfac-jax==0.0.5) (0.23.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.7->kfac-jax==0.0.5) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.7->kfac-jax==0.0.5) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.7->kfac-jax==0.0.5) (1.11.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.8->distrax>=0.1.3->kfac-jax==0.0.5) (4.10.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.8->distrax>=0.1.3->kfac-jax==0.0.5) (0.12.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.15.0->distrax>=0.1.3->kfac-jax==0.0.5) (1.16.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.15.0->distrax>=0.1.3->kfac-jax==0.0.5) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.15.0->distrax>=0.1.3->kfac-jax==0.0.5) (2.2.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.15.0->distrax>=0.1.3->kfac-jax==0.0.5) (0.5.4)\n",
            "Building wheels for collected packages: kfac-jax\n",
            "  Building wheel for kfac-jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kfac-jax: filename=kfac_jax-0.0.5-py3-none-any.whl size=145934 sha256=dd9ba6310eb3f12899bdf5b2e01af06abf2eca7dcfead4a0b6d089c8e8504033\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/00/92/f7138b019299f084dea5bdd41617ff971b8240abe1426cf70c\n",
            "Successfully built kfac-jax\n",
            "Installing collected packages: immutabledict, distrax, kfac-jax\n",
            "Successfully installed distrax-0.1.5 immutabledict-4.2.0 kfac-jax-0.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import Jax libraries\n",
        "\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "\n",
        "import kfac_jax\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.colors as colors\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import time"
      ],
      "metadata": {
        "id": "_Tby3hsjEUWk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rI2zODUmCI4e"
      },
      "outputs": [],
      "source": [
        "d = {}\n",
        "\n",
        "x = jnp.array([160, 192, 256, 320, 384, 448, 480, 512, 544, 576, 608, 640, 672, 736, 800, 864, 928])\n",
        "\n",
        "d[1] = jnp.array([2.1390470027923585, 2.117371106147766, 2.079360914230347, 2.0634059429168703, 2.0418279647827147, 2.0355663776397703, 2.026038479804993, 1.9078317523002624, 2.02663938999176, 1.9916373491287231, 2.0125707626342773, 1.710679531097412, 1.6011452078819275, 0.846069610118866, 0.758564773053006, 0.6476833794649804, 0.5569544512818092])\n",
        "d[2] = jnp.array([2.1376364469528197, 2.1151882171630856, 2.0833156228065492, 2.0677855014801025, 2.0513099908828734, 2.038252031803131, 2.0317952275276183, 1.9605468928813934, 2.004380291700363, 1.8638319730758668, 1.9744050443172452, 1.3116880565881728, 1.7015524208545685, 0.9070954531431198, 0.758564773053006, 0.6476833794649804, 0.5569544512818092])\n",
        "d[4] = jnp.array([2.137095981836319, 2.1224698305130003, 2.088930642604828, 2.0702469706535336, 2.052493894100189, 2.026791015267372, 2.0324197649955753, 1.9859482169151306, 1.9970914155244825, 1.9276028007268906, 1.974685424566269, 1.305090998113155, 1.2910092800855637, 0.9230422914028167, 0.758564773053006, 0.6476833794649804, 0.5569544512818092])\n",
        "d[8] = jnp.array([2.1369586765766146, 2.121186754107475, 2.0904825806617735, 2.069430720806122, 2.0525248765945436, 2.0279675677418707, 2.034416487812996, 1.9994710996747016, 1.9973064497113227, 1.8660007819533349, 1.9134346708655356, 1.4452200539410112, 1.2168424412608148, 0.8034091308712958, 0.758564773053006, 0.6476833794649804, 0.5569544512818092])\n",
        "d[16] = jnp.array([2.1355819702148438, 2.11949605345726, 2.091466601192951, 2.067236562073231, 2.0533283576369286, 2.03372470960021, 2.0313696436584, 1.9970302850008013, 1.8868532452732325, 1.795054268091917, 1.8284942470490932, 1.4987001568078995, 1.2225934054702523, 0.873004288226366, 0.758564773053006, 0.6476833794649804, 0.5569544512818092])\n",
        "d[32] = jnp.array([2.1353486873209477, 2.121116054803133, 2.0907816089689732, 2.0677708223462106, 2.052898390591144, 2.0356978997588158, 2.0278278812766075, 1.9988414000719787, 1.9362491795793177, 1.8008906293660403, 1.7507613642141222, 1.5744482494890688, 1.14803548976779, 0.891236587241292, 0.758564773053006, 0.6476833794649804, 0.5569544512818092])\n",
        "d[64] = jnp.array([2.137277441099286, 2.120296173915267, 2.0904221937060354, 2.068283974006772, 2.053763113170862, 2.0363531768321987, 2.0256495762616398, 2.0059976944699884, 1.9559663173742594, 1.8503267159685493, 1.705769655108452, 1.524729472398758, 1.1931680483743548, 0.924496098794043, 0.758564773053006, 0.6476833794649804, 0.5569544512818092])\n",
        "d[128] = jnp.array([2.135964014567435, 2.119187116064131, 2.0901578743010756, 2.0690575897693635, 2.053141443990171, 2.0379311349242926, 2.0286932651884855, 2.006364729627967, 1.9611748593859377, 1.8800712652504445, 1.6749626398552209, 1.5216509925667197, 1.266984701436013, 0.9443971992935987, 0.758564773053006, 0.6476833794649804, 0.5569544512818092])\n",
        "d[256] = jnp.array([2.1367662544362247, 2.11862374022603, 2.0895097734406587, 2.0692675811238583, 2.0531316876411436, 2.037918024184182, 2.0288481385912744, 2.008296172041446, 1.9522120296023786, 1.8683750097174197, 1.6537404564907776, 1.4866096682846546, 1.2687536558369175, 0.955623487662524, 0.758564773053006, 0.6476833794649804, 0.5569544512818092])\n",
        "d[512] = jnp.array([2.137574161682278, 2.118415069114417, 2.0890740695409473, 2.0693489262834186, 2.0537336371839046, 2.0383666599867865, 2.0280791949713604, 2.0088493178132922, 1.9522541341837496, 1.8633897365187293, 1.6931888234918007, 1.5011318034026773, 1.2837497161002829, 0.9645728270639665, 0.758564773053006, 0.6476833794649804, 0.5569544512818092])\n",
        "d[1024] = jnp.array([2.1381775154266505, 2.118135644472204, 2.0895444331225006, 2.0699078844860193, 2.054174785851501, 2.038330789306201, 2.028024197195191, 2.005973980214913, 1.9558538354118356, 1.863138405947356, 1.708915365703837, 1.5065040085813963, 1.2983537114982027, 0.9589719491399592, 0.758564773053006, 0.6476833794649804, 0.5569544512818092])\n",
        "d[2048] = jnp.array([2.138090460931993, 2.1182356783538125, 2.0895550782467835, 2.0698839781462883, 2.0540498701977157, 2.038370887782952, 2.0281428066222635, 2.004968715856055, 1.9557614909132588, 1.863138405947356, 1.708915365703837, 1.5063766357504969, 1.2975472073862941, 0.9655968356014688, 0.758564773053006, 0.6476833794649804, 0.5569544512818092])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title The functional forms\n",
        "\n",
        "def count_params(params):\n",
        "  return sum([jnp.prod(jnp.arratarget(x.shape))\n",
        "              for x in jax.tree_util.tree_leaves(params)])\n",
        "\n",
        "def bnsl_dim(x, n_breaks, name):\n",
        "  out = nn.Dense(1)(x) + nn.Dense(1, use_bias=False)(nn.softplus(type(name, (nn.Dense,), {})(n_breaks)(x)))\n",
        "  return out\n",
        "\n",
        "class FunctionalForm(nn.Module):\n",
        "  \"\"\"BNSL operating in log-log space (i.e. inputs (and children of inputs) are never explicitly raised to a power).\"\"\"\n",
        "  n_breaks: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, batch_train):\n",
        "    x = jnp.log(x)\n",
        "\n",
        "    offset = 1e-16\n",
        "\n",
        "    x_train, y_train = batch_train\n",
        "    x_train, y_train = jnp.log(x_train), jnp.log(y_train + offset)\n",
        "\n",
        "    x_mean = jnp.mean(x_train, axis=0, keepdims=True)\n",
        "    x_std = jnp.std(x_train, axis=0, keepdims=True)\n",
        "    x_std = jnp.where(x_std == 0., 1., x_std)\n",
        "    x = (x - x_mean)/x_std\n",
        "\n",
        "    y_mean = jnp.mean(y_train)\n",
        "    u = jnp.ones_like(x[:,0]) * y_mean\n",
        "\n",
        "    eps_2 = jnp.log(1e-20)\n",
        "\n",
        "    eps_array = jnp.ones_like(x) * eps_2\n",
        "\n",
        "    x0 = jnp.zeros_like(x)\n",
        "\n",
        "    a = type(\"irr_ent\", (nn.Dense,), {})(1, use_bias=True)(x0)\n",
        "\n",
        "    _mbnsl = jnp.concatenate([\n",
        "          bnsl_dim(x, self.n_breaks, 'x'),\n",
        "          a,\n",
        "          eps_array,\n",
        "      ], axis=-1)\n",
        "\n",
        "    mbnsl = jax.scipy.special.logsumexp(_mbnsl, axis=-1, keepdims=False)\n",
        "\n",
        "    u = mbnsl + u\n",
        "\n",
        "    return jnp.exp(mbnsl)\n",
        "\n",
        "def functional_form__sle(params, _model, batch, batch_train):\n",
        "    x, y = batch\n",
        "    y_pred = _model.apply(params, x, batch_train)\n",
        "    pred, target = jnp.log(y_pred), jnp.log(y)\n",
        "    return jnp.square(pred - target)\n",
        "\n",
        "def functional_form__standard_le(params, _model, batch, batch_train):\n",
        "    x, y = batch\n",
        "    y_pred = _model.apply(params, x, batch_train)\n",
        "    pred, target = jnp.log(y_pred), jnp.log(y)\n",
        "\n",
        "    error = (pred - target) ** 2\n",
        "    err_mu = jnp.mean(error)\n",
        "    std_err = jnp.sqrt(err_mu + jnp.std(error) / (len(pred)**0.5)) - jnp.sqrt(err_mu)\n",
        "\n",
        "    return std_err\n"
      ],
      "metadata": {
        "id": "XCH417DXEvEt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Fits the functional form\n",
        "\n",
        "try:\n",
        "  shutil.rmtree(\"plots\")\n",
        "except:\n",
        "  pass\n",
        "os.mkdir(\"plots\")\n",
        "\n",
        "data_dict = {\n",
        "    \"name\": [],\n",
        "    \"rmsle_train\": [],\n",
        "    \"rmsle_extrap\": [],\n",
        "    \"rmsle_all\": [],\n",
        "    \"standard_le_train\": [],\n",
        "    \"standard_le_all\": [],\n",
        "}\n",
        "\n",
        "\n",
        "for key, value in d.items():\n",
        "    y = value\n",
        "\n",
        "    title = str(\"4 Digit Addition (\")+str(key)+\" \"+str(\"seeds)\")\n",
        "    plot_name = title\n",
        "    plot_name = plot_name.replace(\" \", \"_\")\n",
        "\n",
        "    x_train = []\n",
        "    x_test = []\n",
        "\n",
        "    y_train = []\n",
        "    y_test = []\n",
        "\n",
        "    x_split = 750\n",
        "\n",
        "    for _x, _y in zip(x, y):\n",
        "      if _x < x_split:\n",
        "        x_train.append(_x)\n",
        "        y_train.append(_y)\n",
        "      else:\n",
        "        x_test.append(_x)\n",
        "        y_test.append(_y)\n",
        "\n",
        "    x_train = jnp.expand_dims(jnp.array(x_train), -1)\n",
        "    x_test = jnp.expand_dims(jnp.array(x_test), -1)\n",
        "\n",
        "    y_train = jnp.array(y_train)\n",
        "    y_test = jnp.array(y_test)\n",
        "\n",
        "    x_all = jnp.expand_dims(jnp.array(x), -1)\n",
        "    y_all = y\n",
        "\n",
        "    n_breaks = 1\n",
        "\n",
        "    model = FunctionalForm(n_breaks=n_breaks)\n",
        "    params = model.init(random.PRNGKey(21), x_train, (x_train, y_train))\n",
        "\n",
        "    def loss(params, batch, offset=1e-16):\n",
        "\n",
        "      x, y = batch\n",
        "      y_pred = model.apply(params, x, batch)\n",
        "\n",
        "      pred, target = jnp.log(offset + y_pred), jnp.log(offset + y)\n",
        "\n",
        "      y_mean = jnp.mean(target)\n",
        "      y_std = jnp.std(target)\n",
        "      y_std = jnp.where(y_std == 0., 1., y_std)\n",
        "\n",
        "      target = target / y_std\n",
        "      pred = pred / y_std\n",
        "\n",
        "      kfac_jax.register_squared_error_loss(pred, target)\n",
        "\n",
        "      return jnp.mean(jnp.square(pred - target))\n",
        "    loss_and_grad = jax.value_and_grad(loss, argnums=0)\n",
        "\n",
        "    # using a second order optimizer makes training way faster\n",
        "    optimizer = kfac_jax.Optimizer(\n",
        "      value_and_grad_func=loss_and_grad,\n",
        "      l2_reg=0.0, # 1e-4,\n",
        "      value_func_has_aux=False,\n",
        "      value_func_has_state=False,\n",
        "      value_func_has_rng=False,\n",
        "      use_adaptive_learning_rate=True,\n",
        "      use_adaptive_momentum=True,\n",
        "      use_adaptive_damping=True,\n",
        "      initial_damping=1.0,\n",
        "      multi_device=False,\n",
        "    )\n",
        "\n",
        "    rng = random.PRNGKey(0)\n",
        "    rng, init_rng = random.split(rng)\n",
        "    opt_state = optimizer.init(params, init_rng, (x_train, y_train))\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Fits in <1min on 1 GPU\n",
        "    best_params, best_loss = params, 1e6\n",
        "\n",
        "    for j in range(int(2e1)):\n",
        "      params = model.init(random.PRNGKey(j), x_train, (x_train, y_train))\n",
        "      rng = random.PRNGKey(j)\n",
        "      rng, init_rng = random.split(rng)\n",
        "      opt_state = optimizer.init(params, init_rng, (x_train, y_train))\n",
        "      for i in range(int(2e3)):\n",
        "        if i == 0:\n",
        "          #print(\"j =\", j)\n",
        "          pass\n",
        "        rng, step_rng = jax.random.split(rng)\n",
        "        params, opt_state, stats = optimizer.step(\n",
        "            params, opt_state, step_rng, batch=(x_train, y_train), global_step_int=i)\n",
        "        if jnp.isnan(stats['loss']):\n",
        "          break\n",
        "        if (stats['loss'] < best_loss) and (stats['loss'] > 1e-12):\n",
        "          best_params = jax.tree_map(lambda x: jnp.array(x), params)\n",
        "          best_loss = stats['loss']\n",
        "          if i % 25 == 0:\n",
        "            sle_train = functional_form__sle(best_params, model, (x_train, y_train), (x_train, y_train))\n",
        "            sle_test = functional_form__sle(best_params, model, (x_test, y_test), (x_train, y_train))\n",
        "            #print(f\"{jnp.sqrt(jnp.mean(sle_train)):.5e}\", f\"{jnp.sqrt(jnp.mean(sle_test)):.5e}\", f\"{best_loss:.5e}\", \"   \", f\"{(time.time() - start_time):.5e}\", \"\", j, i)\n",
        "\n",
        "    bc = best_params\n",
        "    print()\n",
        "    print(title)\n",
        "    print(bc)\n",
        "    print(\"time:\", time.time() - start_time)\n",
        "\n",
        "    sle_all = functional_form__sle(bc, model, (x_all, y_all), (x_train, y_train))\n",
        "    sle_train = functional_form__sle(bc, model, (x_train, y_train), (x_train, y_train))\n",
        "    sle_test = functional_form__sle(bc, model, (x_test, y_test), (x_train, y_train))\n",
        "\n",
        "    standard_le_all = functional_form__standard_le(bc, model, (x_all, y_all), (x_train, y_train))\n",
        "    standard_le_train = functional_form__standard_le(bc, model, (x_train, y_train), (x_train, y_train))\n",
        "\n",
        "    print(\"rmsle all:   \", jnp.sqrt(jnp.mean(sle_all)))\n",
        "    print(\"rmsle train: \", jnp.sqrt(jnp.mean(sle_train)))\n",
        "    print(\"rmsle test:  \", jnp.sqrt(jnp.mean(sle_test)))\n",
        "\n",
        "    print(\"standard_le_all:   \", standard_le_all)\n",
        "    print(\"standard_le_train: \", standard_le_train)\n",
        "\n",
        "    data_dict[\"name\"].append(plot_name)\n",
        "    data_dict[\"rmsle_train\"].append(jnp.sqrt(jnp.mean(sle_train)))\n",
        "    data_dict[\"rmsle_extrap\"].append(jnp.sqrt(jnp.mean(sle_test)))\n",
        "    data_dict[\"rmsle_all\"].append(jnp.sqrt(jnp.mean(sle_all)))\n",
        "    data_dict[\"standard_le_train\"].append(standard_le_train)\n",
        "    data_dict[\"standard_le_all\"].append(standard_le_all)\n",
        "\n",
        "    points = 1024\n",
        "    x_tile = jnp.expand_dims(jnp.logspace(-1, 15, points), -1)\n",
        "\n",
        "    pred = model.apply(bc, x_tile, (x_train, y_train))\n",
        "\n",
        "    plt.plot(x_train, y_train, 'o', color='black', markersize=9.1)\n",
        "    plt.plot(x_test, y_test, 'o', color=[0.0, 0.835, 0.0], markersize=9.37, markerfacecolor=[0.0, 1.0, 0.0])\n",
        "    plt.plot(x_tile, pred, color=[1.0, 0.1275, 0.1275], linewidth=2.15, alpha=1.0)\n",
        "\n",
        "    plt.xlim(x.min()*.865,x.max()*1.05)\n",
        "    plt.ylim(y.min()*.9,y.max()*1.05)\n",
        "\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel(\"Training Dataset Size\", fontsize=12)\n",
        "    plt.ylabel(\"Test Cross-Entropy\", fontsize=12)\n",
        "    plt.tick_params(axis='both', labelsize=12)\n",
        "\n",
        "    \"\"\"\n",
        "    plt.xscale('log')\n",
        "    plt.yscale('log')\n",
        "    #\"\"\"\n",
        "\n",
        "    plt.savefig('plots/'+plot_name+'.png', bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "df = pd.DataFrame(data_dict)\n",
        "df.to_csv(\"error.csv\")\n",
        "\n",
        "!zip -r plots.zip plots/"
      ],
      "metadata": {
        "id": "-SIC2SkKE0PR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}